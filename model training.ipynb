{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gsevr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gsevr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, Sequential\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import tensorflow_hub as hub\n",
    "from tokenization import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('IMDB dataset.csv')\n",
    "# df['content'] = df.content.astype(str)\n",
    "# df['sentiment'] = df.sentiment.astype(str)\n",
    "# df['sentiment'] = df.sentiment.apply(lambda x: 'negative' if x == 'empty' or x == 'sadness' or x == 'worry' or x == 'hate' or x == 'boredom' or x == 'anger' \\\n",
    "#                         else 'neutral' if x == 'neutral' \\\n",
    "#                             else 'positive')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df.review.apply(lambda x: re.sub(r'@[^ ]*',r'',x))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    t = text.lower()\n",
    "    # t = re.sub('_',r'',t)\n",
    "    # t = re.sub('\\d+',r'',t)\n",
    "    t = re.sub(r'@[^ ]*',r'',t)\n",
    "    t = re.sub(r'\\W+',r' ',t)\n",
    "    t = re.sub(r'(https|quot|http)', '', t)\n",
    "    t = re.sub(r'\\b(?!(?:ai)\\b)\\w{3}\\b','', t)\n",
    "    t = re.sub(r'http?://\\S+|www\\.\\S+','',t)\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    txt = ' '.join([word for word in t.split() if word not in stopwords_list])\n",
    "    return txt\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "df['prepro'] = [' '.join([lemmatizer.lemmatize(preprocess(email))])\n",
    "                 .strip() for email in df['review']]\n",
    "\n",
    "\n",
    "texts = df.prepro.values\n",
    "labels = df.sentiment.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(categories=[['positive','negative']])\n",
    "labels = ohe.fit_transform(df.sentiment.to_numpy().reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load GloVe embeddings (50-dimensional vectors in this example)\n",
    "glove_path = 'glove.6B.300d.txt'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(glove_path, binary=False, encoding='utf8',no_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_length = 100  # Set your desired sequence length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_dim = 300  # Use the same dimension as your pretrained embeddings\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word_vectors:\n",
    "        embedding_matrix[i] = word_vectors[word]\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=len(tokenizer.word_index) + 1,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=100,\n",
    "    trainable=False  # Set to True if you want to fine-tune embeddings\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(layers.Dropout(0.4))\n",
    "# model.add(layers.Conv1D(32,3, padding='same', activation='relu'))\n",
    "# model.add(layers.MaxPool1D())\n",
    "model.add(layers.Reshape((-1,300)))\n",
    "model.add(layers.Bidirectional(layers.GRU(128,activation='tanh')))\n",
    "# layers.Dropout(0.4),\n",
    "# layers.Dense(64,activation='relu',kernel_initializer=\"he_normal\"),\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenizer.texts_to_sequences(X_test)\n",
    "test_sequences = pad_sequences(test, maxlen=100, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1172/1172 [==============================] - 104s 86ms/step - loss: 0.5669 - accuracy: 0.6930 - val_loss: 0.4316 - val_accuracy: 0.8139\n",
      "Epoch 2/20\n",
      "1172/1172 [==============================] - 128s 109ms/step - loss: 0.4192 - accuracy: 0.8130 - val_loss: 0.4139 - val_accuracy: 0.8306\n",
      "Epoch 3/20\n",
      "1172/1172 [==============================] - 143s 122ms/step - loss: 0.3885 - accuracy: 0.8304 - val_loss: 0.4335 - val_accuracy: 0.8363\n",
      "Epoch 4/20\n",
      "1172/1172 [==============================] - 147s 125ms/step - loss: 0.3697 - accuracy: 0.8377 - val_loss: 0.4019 - val_accuracy: 0.8462\n",
      "Epoch 5/20\n",
      "1172/1172 [==============================] - 148s 126ms/step - loss: 0.3580 - accuracy: 0.8420 - val_loss: 0.4571 - val_accuracy: 0.8377\n",
      "Epoch 6/20\n",
      "1172/1172 [==============================] - 147s 126ms/step - loss: 0.3423 - accuracy: 0.8520 - val_loss: 0.3405 - val_accuracy: 0.8602\n",
      "Epoch 7/20\n",
      "1172/1172 [==============================] - 144s 123ms/step - loss: 0.3296 - accuracy: 0.8561 - val_loss: 0.3454 - val_accuracy: 0.8606\n",
      "Epoch 8/20\n",
      "1172/1172 [==============================] - 149s 127ms/step - loss: 0.3199 - accuracy: 0.8617 - val_loss: 0.3501 - val_accuracy: 0.8610\n",
      "Epoch 9/20\n",
      "1172/1172 [==============================] - 135s 115ms/step - loss: 0.3120 - accuracy: 0.8649 - val_loss: 0.3490 - val_accuracy: 0.8679\n",
      "Epoch 10/20\n",
      "1172/1172 [==============================] - 139s 118ms/step - loss: 0.3063 - accuracy: 0.8700 - val_loss: 0.3485 - val_accuracy: 0.8646\n",
      "Epoch 11/20\n",
      "1172/1172 [==============================] - 139s 119ms/step - loss: 0.2945 - accuracy: 0.8765 - val_loss: 0.3216 - val_accuracy: 0.8745\n",
      "Epoch 12/20\n",
      "1172/1172 [==============================] - 135s 115ms/step - loss: 0.2928 - accuracy: 0.8759 - val_loss: 0.3332 - val_accuracy: 0.8713\n",
      "Epoch 13/20\n",
      "1172/1172 [==============================] - 144s 123ms/step - loss: 0.2870 - accuracy: 0.8778 - val_loss: 0.3517 - val_accuracy: 0.8610\n",
      "Epoch 14/20\n",
      "1172/1172 [==============================] - 141s 121ms/step - loss: 0.2771 - accuracy: 0.8821 - val_loss: 0.3179 - val_accuracy: 0.8742\n",
      "Epoch 15/20\n",
      "1172/1172 [==============================] - 139s 118ms/step - loss: 0.2731 - accuracy: 0.8851 - val_loss: 0.3299 - val_accuracy: 0.8764\n",
      "Epoch 16/20\n",
      "1172/1172 [==============================] - 147s 125ms/step - loss: 0.2656 - accuracy: 0.8889 - val_loss: 0.3565 - val_accuracy: 0.8790\n",
      "Epoch 17/20\n",
      "1172/1172 [==============================] - 148s 126ms/step - loss: 0.2608 - accuracy: 0.8902 - val_loss: 0.3167 - val_accuracy: 0.8786\n",
      "Epoch 18/20\n",
      "1172/1172 [==============================] - 146s 124ms/step - loss: 0.2549 - accuracy: 0.8945 - val_loss: 0.3183 - val_accuracy: 0.8819\n",
      "Epoch 19/20\n",
      "1172/1172 [==============================] - 142s 121ms/step - loss: 0.2490 - accuracy: 0.8975 - val_loss: 0.3009 - val_accuracy: 0.8822\n",
      "Epoch 20/20\n",
      "1172/1172 [==============================] - 145s 124ms/step - loss: 0.2452 - accuracy: 0.8976 - val_loss: 0.3401 - val_accuracy: 0.8837\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_sequences, y_train, epochs=20, validation_data=(test_sequences, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['i feel excited about my doctor appointment today']\n",
    "classes = ['positive', 'negative']\n",
    "\n",
    "def predict(text):\n",
    "    test_seq = tokenizer.texts_to_sequences(text)\n",
    "    pad_test = pad_sequences(test_seq, maxlen=max_sequence_length, padding='post')\n",
    "    model_prediction = model.predict(pad_test)\n",
    "    prediction = classes[np.argmax(model_prediction)]\n",
    "    return prediction\n",
    "\n",
    "# prediction = model.predict(pad_test)\n",
    "# df.sentiment.unique()[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
